{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from invoice_parser.imports import *\n",
    "from invoice_parser.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Invoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def page0_text(pdf):\n",
    "    loaded_pdf = PdfReader(pdf)\n",
    "    p = loaded_pdf.pages[0]\n",
    "    return p.extract_text()\n",
    "\n",
    "\n",
    "def is_invoice_text(text, model):\n",
    "    return model(text).detach().cpu().item() == 0\n",
    "\n",
    "\n",
    "def is_invoice(pdf, model, device=None):\n",
    "    if model is None:\n",
    "        model = load_invoice_model(device=device)\n",
    "    return is_invoice_text(page0_text(pdf), model)\n",
    "\n",
    "\n",
    "def is_invoice_chain(\n",
    "    model,\n",
    "    device=None,\n",
    "    input_variables=[\"pdf\"],\n",
    "    output_variables=[\"is_invoice\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    return transform_chain(\n",
    "        is_invoice,\n",
    "        transform_kwargs={\"model\": model, \"device\": device},\n",
    "        vars_kwargs_mapping={input_variables[0]: \"pdf\"},\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "vline_settings = {\n",
    "    \"horizontal_strategy\": \"text\",\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "hline_settings = {\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"vertical_strategy\": \"text\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "line_settings = {\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "text_settings = {\n",
    "    \"horizontal_strategy\": \"text\",\n",
    "    \"vertical_strategy\": \"text\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "text_settings = {\n",
    "    # \"intersection_x_tolerance\": 5,\n",
    "    # \"snap_y_tolerance\": 5,\n",
    "    # \"join_x_tolerance\": 5,\n",
    "    # \"join_y_tolerance\": 5,\n",
    "    \"text_layout\": True\n",
    "}\n",
    "\n",
    "\n",
    "def get_fullest_row(table):\n",
    "    rows = [r for r in table if full_row(r)]\n",
    "    if len(rows) == 0:\n",
    "        rows = table\n",
    "    row = max(rows, key=len)\n",
    "    return row, table.index(row)\n",
    "\n",
    "\n",
    "def num_full_parts(row):\n",
    "    return len([p for p in row if not empty_part(p)])\n",
    "\n",
    "\n",
    "def get_table_items(table):\n",
    "    if table is None or len(table) == 0:\n",
    "        return []\n",
    "\n",
    "    cols, cols_idx = get_fullest_row(table)\n",
    "    for i, c in enumerate(cols):\n",
    "        if empty_part(c):\n",
    "            cols[i] = f\"col_{i}\"\n",
    "\n",
    "    # let's assume that the first full row after the cols row is the first item\n",
    "    first_order_row_idx = get_first_full_row(table[cols_idx + 1 :])[1]\n",
    "    if first_order_row_idx is None:\n",
    "        first_order_row_idx = get_first_non_empty_row(table[cols_idx + 1 :])[1]\n",
    "    if first_order_row_idx is None:\n",
    "        first_order_row_idx = 0\n",
    "    first_order_row_idx += cols_idx + 1\n",
    "\n",
    "    items = []\n",
    "    item = {c: \"\" for c in cols}\n",
    "    first_order_row_idx = min(first_order_row_idx, len(table) - 1)\n",
    "    order_table = table[first_order_row_idx:]\n",
    "    curr_row_len = num_full_parts(order_table[0])\n",
    "    for row in order_table:\n",
    "        if ((num_full_parts(row) == curr_row_len) or empty_row(row)) and len(item) > 0:\n",
    "            items.append(item)\n",
    "            item = {c: \"\" for c in cols}\n",
    "            if not empty_row(row):\n",
    "                curr_row_len = num_full_parts(row)\n",
    "        for i, c in enumerate(cols):\n",
    "            row_part = row[i]\n",
    "            if not empty_part(row_part):\n",
    "                row_part = \" \".join(row[i].split(\"\\n\"))\n",
    "                item[c] += row_part + \" \"\n",
    "    items.append(item)\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def row_check(row, target_list, target_thresh=2):\n",
    "    \"\"\"\n",
    "    Checks if the given row contains the target elements.\n",
    "\n",
    "    Parameters:\n",
    "        row (str): A string representing the row to check.\n",
    "        target_list (list): A list of strings representing the target elements.\n",
    "        target_thresh (int): The minimum number of target elements that must be present in the row.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the row contains the target elements, False otherwise.\n",
    "    \"\"\"\n",
    "    check_list = [hc for hc in target_list if hc.lower() in row.strip().lower()]\n",
    "    return len(check_list) >= target_thresh\n",
    "\n",
    "\n",
    "def extract_sub_text(\n",
    "    text,\n",
    "    top_cols,\n",
    "    bottom_cols,\n",
    "    top_thresh=2,\n",
    "    bottom_thresh=1,\n",
    "    alt_top_index=0,\n",
    "    alt_bottom_index=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts the text between the top_cols and bottom_cols.\n",
    "    \"\"\"\n",
    "    top_idx = find_target_index(\n",
    "        text, top_cols, target_thresh=top_thresh, alt_index=alt_top_index\n",
    "    )\n",
    "    bottom_idx = find_target_index(\n",
    "        text[::-1], bottom_cols, target_thresh=bottom_thresh, alt_index=alt_bottom_index\n",
    "    )\n",
    "    bottom_idx = len(text) - bottom_idx - 1\n",
    "    return (\n",
    "        [t for t in text[top_idx : bottom_idx + 1] if len(t.strip()) > 0],\n",
    "        top_idx,\n",
    "        bottom_idx,\n",
    "    )\n",
    "\n",
    "\n",
    "def find_target_index(data, target_list, target_thresh=2, alt_index=0):\n",
    "    \"\"\"\n",
    "    Finds the index of the row in the given data that contains the target elements.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): A list of strings representing the data.\n",
    "        target_list (list): A list of strings representing the target elements.\n",
    "        target_thresh (int): The minimum number of target elements that must be present in a row.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the row that contains the target elements. If no such row exists, returns alt_index.\n",
    "    \"\"\"\n",
    "    target_idx = None\n",
    "    for idx, row in enumerate(data):\n",
    "        if row_check(row, target_list, target_thresh):\n",
    "            target_idx = idx\n",
    "            break\n",
    "    if target_idx is None:\n",
    "        msg.warn(f\"No target found in data. Setting it to {alt_index}.\", spaced=True)\n",
    "        target_idx = alt_index\n",
    "    return target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def json_str(x):\n",
    "    x = x[x.find(\"{\") : x.rfind(\"}\") + 1]\n",
    "    x = x.splitlines()\n",
    "    jstr = [x[0]]\n",
    "    for s in x[1:]:\n",
    "        if \":\" not in s and s.strip() != \"}\":\n",
    "            jstr[-1] = jstr[-1][:-2] + \" \" + s[1:]\n",
    "        else:\n",
    "            jstr.append(s)\n",
    "    jstr = \" \".join(jstr).strip()\n",
    "    if jstr[-1] in [\",\", \";\"]:\n",
    "        jstr = jstr[:-1].strip()\n",
    "    jstr = jstr.replace(\",}\", \"}\")\n",
    "    jstr = re.sub(r\"\\s+\", \" \", jstr)\n",
    "    jstr = jstr.replace('\" ', '\"')\n",
    "    jstr = jstr.replace(' \"', '\"')\n",
    "    jstr = jstr.replace(\", }\", \"}\")\n",
    "    jstr = re.sub(r\"[a-zA-Z0-9]\\}\", '\"}', jstr)\n",
    "    jstr = re.sub(r\":\\s*(\\w)\", r': \"\\1', jstr)\n",
    "    jstr = re.sub(r\"\\b0+(\\d+)\\b\", r\"\\1\", jstr)\n",
    "    # jstr = re.sub(r\"\\s*([{}])\\s*\", r\"\\1\", jstr)\n",
    "    jstr = re.sub(r\"\\s*([:,])\\s*\", r\"\\1 \", jstr)\n",
    "    jstr = re.sub(r\",}\", '\"}', jstr)\n",
    "    jstr = re.sub(r\"\\\"+}\", '\"}', jstr)\n",
    "    jstr = jstr.replace('\"\"', '\",\"')\n",
    "    return jstr\n",
    "\n",
    "\n",
    "def str_to_json(x, max_try=10):\n",
    "    # jstr = json_str(x)\n",
    "    jstr = x[x.find(\"{\") : x.rfind(\"}\") + 1]\n",
    "    jstr = jstr.replace(': \",', ': \"\",')\n",
    "    json_dict = {}\n",
    "    tries = 0\n",
    "    while True and tries < max_try:\n",
    "        try:\n",
    "            json_dict = json.loads(jstr)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            unexp = int(re.findall(r\"\\(char (\\d+)\\)\", str(e))[0])\n",
    "            unesc = jstr.rfind(r'\"', 0, unexp)\n",
    "            jstr = jstr[:unesc] + r\"\\\"\" + jstr[unesc + 1 :]\n",
    "            closg = jstr.find(r'\"', unesc + 2)\n",
    "            jstr = jstr[:closg] + r\"\\\"\" + jstr[closg + 1 :]\n",
    "            tries += 1\n",
    "    return jstr, json_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def extract_text(path):\n",
    "    data = pdfplumber.open(path)\n",
    "    pdf_text = [p.extract_text(layout=True).splitlines() for p in data.pages]\n",
    "    text = pdf_text[0]\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_order_docs(\n",
    "    text,\n",
    "    header_cols=[\"item\", \"description\", \"price\", \"quantity\", \"amount\", \"total\", \"qty\"],\n",
    "    get_parts=True,\n",
    "    splitter=None,\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "):\n",
    "    msg.info(\"Extracting ORDER docs from PDF.\", spaced=True)\n",
    "\n",
    "    if splitter is None:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "    avg_len = mode([len(t) for t in text])\n",
    "    order_text = [text[0]]\n",
    "    order_metadatas = [{}]\n",
    "    desc = \"\"\n",
    "    # for i, txt in enumerate(text[1:-1], start=1):\n",
    "    for txt in text[1:-1]:\n",
    "        if not len(txt) >= avg_len * 2:\n",
    "            txt = txt.replace('\"', \"\").replace(\"'\", \"\").strip()\n",
    "            if row_check(txt, header_cols, 2):\n",
    "                order_text.append(txt)\n",
    "                order_metadatas.append({})\n",
    "            elif len(txt) < avg_len * 0.75 and not row_check(\n",
    "                order_text[-1], header_cols, 2\n",
    "            ):\n",
    "                desc += \" \" + txt\n",
    "            else:\n",
    "                if len(desc) > 0:\n",
    "                    order_metadatas.append({\"desc\": desc.strip()})\n",
    "                    desc = \"\"\n",
    "                if get_parts:\n",
    "                    part_nums = [\n",
    "                        x\n",
    "                        for x in re.findall(r\"\\d{5}\", txt)\n",
    "                        if not x.startswith(\"00\") and \".\" not in x\n",
    "                    ]\n",
    "                    if len(part_nums) == 0:\n",
    "                        part_nums = [\n",
    "                            x\n",
    "                            for x in re.findall(r\"\\d{4}\", txt)\n",
    "                            if not x.startswith(\"00\") and \".\" not in x\n",
    "                        ]\n",
    "                    if len(part_nums) > 0:\n",
    "                        part_num = part_nums[0]\n",
    "                        txt += f\" part_number: {part_num}\"\n",
    "                order_text.append(txt)\n",
    "    if len(desc) > 0:\n",
    "        order_metadatas.append({\"desc\": desc.strip()})\n",
    "    order_text.append(text[-1])\n",
    "    order_metadatas += [{} for _ in range(len(order_text) - len(order_metadatas))]\n",
    "    docs = splitter.create_documents(order_text, metadatas=order_metadatas)\n",
    "    msg.good(\"ORDER docs extracted.\", spaced=True)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def info_order_docs(\n",
    "    text,\n",
    "    header_cols=[\"item\", \"description\", \"price\", \"quantity\", \"amount\", \"total\", \"qty\"],\n",
    "    total_cols=[\"total\", \"subtotal\", \"tax\"],\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "    get_parts=True,\n",
    "):\n",
    "    msg.info(\"Extracting INFO and ORDER docs from PDF.\", spaced=True)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    table_text, top_idx, bottom_idx = extract_sub_text(\n",
    "        text,\n",
    "        top_cols=header_cols,\n",
    "        bottom_cols=total_cols,\n",
    "        top_thresh=2,\n",
    "        bottom_thresh=1,\n",
    "        alt_top_index=0,\n",
    "        alt_bottom_index=0,\n",
    "    )\n",
    "\n",
    "    top_text = [\n",
    "        t.replace('\"', \"\").replace(\"'\", \"\").strip()\n",
    "        for t in text[:top_idx]\n",
    "        if len(t.strip()) > 0\n",
    "    ]\n",
    "    bottom_text = [\n",
    "        t.replace('\"', \"\").replace(\"'\", \"\").strip()\n",
    "        for t in text[bottom_idx:]\n",
    "        if len(t.strip()) > 0\n",
    "    ]\n",
    "    info_text = top_text + bottom_text[1:]\n",
    "    all_info_text = \" \".join(info_text)\n",
    "    qns = [x for x in re.findall(r\"\\d{8}\", all_info_text) if x.startswith(\"2\")]\n",
    "    if len(qns) > 0:\n",
    "        info_text.insert(0, f\"QOUTE NUMBER: {qns[0]}\")\n",
    "    info_docs = splitter.create_documents(info_text)\n",
    "    order_docs = extract_order_docs(\n",
    "        table_text, header_cols=header_cols, get_parts=get_parts, splitter=splitter\n",
    "    )\n",
    "    bottom_docs = splitter.create_documents(bottom_text[:1])\n",
    "    msg.good(\"INFO and ORDER docs extracted.\", spaced=True)\n",
    "\n",
    "    return dict(info_docs=info_docs, order_docs=order_docs, bottom_docs=bottom_docs)\n",
    "\n",
    "\n",
    "def pdf_to_info_order_docs(\n",
    "    path,\n",
    "    header_cols=[\"item\", \"description\", \"price\", \"quantity\", \"amount\", \"total\", \"qty\"],\n",
    "    total_cols=[\"total\", \"subtotal\", \"tax\"],\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "    get_parts=True,\n",
    "):\n",
    "    text = extract_text(path)\n",
    "    res = info_order_docs(\n",
    "        text,\n",
    "        header_cols=header_cols,\n",
    "        total_cols=total_cols,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        get_parts=get_parts,\n",
    "    )\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = default_device()\n",
    "\n",
    "# embeddings = SentenceTransformerEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "#     model_kwargs={\"device\": default_device()},\n",
    "# )\n",
    "# model = \"tiiuae/falcon-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def qa_llm_chain(model=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    token = \"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\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model, device_map=\"auto\", torch_dtype=torch.float16, token=token\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0})\n",
    "    return load_qa_chain(llm, \"stuff\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def fix_json(text):\n",
    "    json_str = copy.deepcopy(text)\n",
    "    if json_str.count(\"{\") != json_str.count(\"}\"):\n",
    "        while json_str.count(\"{\") % 2 != 0:\n",
    "            json_str = \"{\" + json_str\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "        while json_str.count(\"}\") % 2 != 0:\n",
    "            json_str = json_str + \"}\"\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "    if json_str.count(\"{\") < json_str.count(\"}\"):\n",
    "        while json_str.count(\"{\") != json_str.count(\"}\"):\n",
    "            json_str = \"{\" + json_str\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "    elif json_str.count(\"{\") > json_str.count(\"}\"):\n",
    "        while json_str.count(\"{\") != json_str.count(\"}\"):\n",
    "            json_str = json_str + \"}\"\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "    json_str = json_str.replace(\"\\n\", \"\")\n",
    "    if json_str.startswith(\"{{\") and json_str.endswith(\"}}\"):\n",
    "        json_str = json_str[1:-1]\n",
    "    json_str = json_str.replace('\"\"\"', '\"')\n",
    "    json_str = json_str.replace('\"\"', '\",\"')\n",
    "    return json_str\n",
    "\n",
    "def check_ends(res, chain, docs, query, start=\"{\", end=\"}\", max_tries=3):\n",
    "    tries = 0\n",
    "    if res is None or res == \"\":\n",
    "        res = \"\"\n",
    "    else:\n",
    "        res = res[res.find(start) : res.rfind(end) + 1]\n",
    "    while res == \"\" and tries < max_tries:\n",
    "        msg.info(f\"Tries: {tries}\", spaced=True)\n",
    "        res = chain(dict(input_documents=docs, question=query))\n",
    "        res = res[\"output_text\"].strip()\n",
    "        res = res[res.find(start) : res.rfind(end) + 1]\n",
    "        tries += 1\n",
    "    return res\n",
    "\n",
    "def line_to_dict(line):\n",
    "    line_find = line[line.find(\"{\")+1 : line.rfind(\"}\")]\n",
    "    if len(line_find) == 0:\n",
    "        line = line.replace('{','').replace(\"}\",'')\n",
    "    else:\n",
    "        line = line_find\n",
    "    k = line.split(\":\")[0].strip()\n",
    "    v = \":\".join(line.split(\":\")[1:]).strip()\n",
    "    return {k: v} if len(k) > 0 and len(v) > 0 else {}\n",
    "\n",
    "def info_res_to_dict(res):\n",
    "    res_dict = {}\n",
    "    for line in res.splitlines():\n",
    "        res_dict.update(line_to_dict(line))\n",
    "        # line = line[line.find(\"{\")+1 : line.rfind(\"}\")]\n",
    "        # k = line.split(\":\")[0].strip()\n",
    "        # v = \":\".join(line.split(\":\")[1:]).strip()\n",
    "        # res_dict[k] = v\n",
    "    return res_dict\n",
    "\n",
    "def order_res_to_list(res):\n",
    "    sep = \"} {\"\n",
    "    res = [[x for x in r.strip().split(sep) if len(x.strip()) > 0] for r in res.splitlines() if len(r.strip()) > 0]\n",
    "    order_list = []\n",
    "    for r in res:\n",
    "        order_dict = {}\n",
    "        for x in r:\n",
    "            order_dict.update(line_to_dict(x))\n",
    "        if len(order_dict) > 0:\n",
    "            order_list.append(order_dict)\n",
    "    return order_list\n",
    "\n",
    "def extract_data(chain, docs, query, format_query='', start=\"{\", end=\"}\", max_tries=3):\n",
    "    # info_query = \"You are a data extractor. Extract the order information like the numbers, dates, and shipping address. Include the quote number too if found.\"\n",
    "    if format_query == '':\n",
    "        format_query = \"\\nReturn the text in the format: {key: value}.\"\n",
    "    suffix = \"\\nDon't tell me how to do it, just do it. Don't add any disclaimer.\"\n",
    "    query += format_query+suffix\n",
    "    query = query.strip()\n",
    "    res = chain(dict(input_documents=docs, question=query))['output_text']\n",
    "    res = res.replace(\"{key: value}\", '').strip()\n",
    "    pprint(res)\n",
    "    res = check_ends(res, chain, docs, query, start=start, end=end, max_tries=max_tries)\n",
    "    return res\n",
    "\n",
    "def extract_info_dict(chain, docs, max_tries=3):\n",
    "    info_query = \"You are a data extractor. Extract the order information like the numbers, dates, and shipping address. Include the quote number too if found.\"\n",
    "    res = extract_data(chain, docs, info_query, max_tries=max_tries)\n",
    "    return info_res_to_dict(res)\n",
    "\n",
    "def extract_order_list(chain, docs, get_parts=True, max_tries=3):\n",
    "    order_query = \"You are a data extractor. Extract the order items with full details and descriptions and prices.\"\n",
    "    part_query = \"Include the part numbers if defined.\"\n",
    "    if get_parts:\n",
    "        order_query += \" \" + part_query\n",
    "    format_query = \"\\nFor each item, return the columns in the format: {key: value}.\"\n",
    "    res = extract_data(chain, docs, order_query, format_query=format_query, max_tries=max_tries)\n",
    "    return order_res_to_list(res)\n",
    "\n",
    "def extract_total_dict(chain, docs, max_tries=3):\n",
    "    total_query = \"You are a data extractor. Extract the total price.\"\n",
    "    res = extract_data(chain, docs, total_query, max_tries=max_tries)\n",
    "    return info_res_to_dict(res)\n",
    "\n",
    "def json_response(chain, docs, query, max_tries=6):\n",
    "    msg.info(\"Converting DOCS to JSON.\", spaced=True)\n",
    "    overall_tries = 0\n",
    "    while overall_tries < max_tries / 2:\n",
    "        tries = 0\n",
    "        res = \"\"\n",
    "        while res == \"\" and tries < max_tries:\n",
    "            msg.info(f\"Tries: {tries}\", spaced=True)\n",
    "            res = chain(dict(input_documents=docs, question=query))\n",
    "            res = res[\"output_text\"].strip()\n",
    "            res = res[res.find(\"{\") : res.rfind(\"}\") + 1]\n",
    "            tries += 1\n",
    "        tries = 0\n",
    "        while tries < max_tries:\n",
    "            msg.info(f\"Tries: {tries}\", spaced=True)\n",
    "            try:\n",
    "                json_res = dict(json_str=res, json=json.loads(fix_json(res)))\n",
    "                msg.good(\"DOCS converted to JSON.\", spaced=True)\n",
    "                return json_res\n",
    "            except:\n",
    "                tries += 1\n",
    "        overall_tries += 1\n",
    "    msg.fail(\"Failed to convert DOCS to JSON.\", spaced=True)\n",
    "    return dict(json_str=res, json={})\n",
    "\n",
    "\n",
    "def info_json(chain, info_docs, max_tries=6):\n",
    "    msg.info(\"Extracting INFO JSON.\", spaced=True)\n",
    "    info_query = \"You are a data extractor. Extract the order information like the numbers, dates, and shipping address. Include the quote number too if found.\"\n",
    "    json_query = \"\\nReturn the text in JSON format. It must be compatible with json.loads.\"\n",
    "    suffix = \"\\nDon't tell me how to do it, just do it. Don't add any disclaimer.\"\n",
    "    info_query += json_query + suffix\n",
    "    res = json_response(chain, info_docs, info_query, max_tries)\n",
    "    res['json'] = {k:v for k,v in res['json'].items() if 'amount' not in k.lower() and 'total' not in k.lower() and 'price' not in k.lower()}\n",
    "    msg.info(f'INFO JSON:\\n{res[\"json\"]}', spaced=True)\n",
    "    msg.good(\"INFO JSON extracted.\", spaced=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def order_json(\n",
    "    chain,\n",
    "    order_docs,\n",
    "    max_tries=6,\n",
    "    get_parts=True,\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "):\n",
    "    msg.info(\"Extracting ORDER JSON.\", spaced=True)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    json_query = \"\\nReturn the text in JSON format. It must be compatible with json.loads.\"\n",
    "    suffix = \"\\nDon't tell me how to do it, just do it. Don't add any disclaimer.\"\n",
    "    part_query = \"Include the part numbers only if explicitly defined.\"\n",
    "    query = \"You are a data extractor. Extract the order items with full details and descriptions and prices. You must include the total price too.\"\n",
    "    if get_parts:\n",
    "        query += \" \" + part_query\n",
    "    query += suffix\n",
    "    query = query.strip()\n",
    "    items = chain(dict(input_documents=order_docs, question=query))[\"output_text\"].strip()\n",
    "\n",
    "    item_query = json_query\n",
    "    if get_parts:\n",
    "        item_query += \" \" + part_query\n",
    "    item_query += suffix\n",
    "    item_query = item_query.strip()\n",
    "\n",
    "    item_docs = splitter.create_documents([items])\n",
    "    res = json_response(chain, item_docs, item_query, max_tries)\n",
    "    msg.good(\"ORDER JSON extracted.\", spaced=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def pdf_to_info_order_json(path, chain, max_tries=6, get_parts=True):\n",
    "    info_order_dict = pdf_to_info_order_docs(path, get_parts=get_parts)\n",
    "    info_dict = info_json(\n",
    "        chain=chain, info_docs=info_order_dict[\"info_docs\"], max_tries=max_tries\n",
    "    )\n",
    "    order_dict = order_json(\n",
    "        chain=chain,\n",
    "        order_docs=info_order_dict[\"order_docs\"],\n",
    "        max_tries=max_tries,\n",
    "        get_parts=get_parts,\n",
    "    )\n",
    "    return {\"info\": info_dict, \"order\": order_dict}\n",
    "\n",
    "def pdf_to_data_json(path, chain, max_tries=3, get_parts=True):\n",
    "    info_order_dict = pdf_to_info_order_docs(path, get_parts=get_parts)\n",
    "    info_dict = extract_info_dict(chain, info_order_dict[\"info_docs\"], max_tries=max_tries)\n",
    "    order_list = extract_order_list(chain, info_order_dict[\"order_docs\"], get_parts=get_parts, max_tries=max_tries)\n",
    "    total_dict = extract_total_dict(chain, info_order_dict[\"bottom_docs\"], max_tries=max_tries*2)\n",
    "    return {\"info\": info_dict, \"order\": order_list, \"total\": total_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a56f4f7394a1ca925ecf1dc953bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "llm_chain = qa_llm_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "data_path = Path(\"/home/hamza/demo_files/ap/\")\n",
    "data_path = Path(\"/home/hamza/demo_files/pdf\")\n",
    "file_name = \"wt4.pdf\"\n",
    "pdf = data_path / file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Extracting INFO and ORDER docs from PDF.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Extracting ORDER docs from PDF.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ ORDER docs extracted.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ INFO and ORDER docs extracted.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/miniconda3/envs/chains/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The answer to your question is already in the provided text. The order '\n",
      " 'information can be found in the following places:\\n'\n",
      " '\\n'\n",
      " '{order number: 51020}\\n'\n",
      " '{date: 7/11/2023}\\n'\n",
      " '{shipping address: 3900 EAST MAIN, SPOKANE, WA 99202}\\n'\n",
      " '{terms: NET 30}\\n'\n",
      " '{vendor code: WILSON TOOL}\\n'\n",
      " '{vendor phone: 651-286-6125}\\n'\n",
      " '{purchasing agent: VANDERPOOL, BRYAN}\\n'\n",
      " '{job number: STOCK}\\n'\n",
      " '{quote number: 21652926}\\n'\n",
      " '\\n'\n",
      " \"Note: The order information is present in the text you provided, you don't \"\n",
      " 'need to perform any action to extract it.')\n",
      "('| Order Items | Description | Cost Unit | Amount | Part Number |\\n'\n",
      " '| --- | --- | --- | --- | --- |\\n'\n",
      " '| 12961 | Station Thick Slug Hugger 2 Die Shape SD | $93.50 EA | $93.50 | '\n",
      " '12961 |\\n'\n",
      " '| 12961 | ATTN: ALEX - 12961 B [1-1/4] Station Thick Slug Hugger 2 Die Shape '\n",
      " 'SD | $93.50 EA | $93.50 | 12961 |\\n'\n",
      " '| 12964 | Station Thick Slug Hugger 2 Die Shape SD | $213.25 EA | $213.25 | '\n",
      " '12964 |\\n'\n",
      " '| 12964 | C [2] Station Thick Slug Hugger 2 Die Shape SD | $213.25 EA | '\n",
      " '$213.25 | 12964 |\\n'\n",
      " '\\n'\n",
      " 'Note: In the output, part numbers are included only if they were defined in '\n",
      " 'the input.')\n",
      "\n",
      "\u001b[38;5;4mℹ Tries: 0\u001b[0m\n",
      "\n",
      "'Total Price:    $306.75'\n",
      "\n",
      "\u001b[38;5;4mℹ Tries: 0\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "data = pdf_to_data_json(pdf, llm_chain, max_tries=3, get_parts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
