{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from invoice_parser.imports import *\n",
    "from invoice_parser.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Invoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def page0_text(pdf):\n",
    "    loaded_pdf = PdfReader(pdf)\n",
    "    p = loaded_pdf.pages[0]\n",
    "    return p.extract_text()\n",
    "\n",
    "\n",
    "def is_invoice_text(text, model):\n",
    "    return model(text).detach().cpu().item() == 0\n",
    "\n",
    "\n",
    "def is_invoice(pdf, model, device=None):\n",
    "    if model is None:\n",
    "        model = load_invoice_model(device=device)\n",
    "    return is_invoice_text(page0_text(pdf), model)\n",
    "\n",
    "\n",
    "def is_invoice_chain(\n",
    "    model,\n",
    "    device=None,\n",
    "    input_variables=[\"pdf\"],\n",
    "    output_variables=[\"is_invoice\"],\n",
    "    verbose=False,\n",
    "):\n",
    "    return transform_chain(\n",
    "        is_invoice,\n",
    "        transform_kwargs={\"model\": model, \"device\": device},\n",
    "        vars_kwargs_mapping={input_variables[0]: \"pdf\"},\n",
    "        input_variables=input_variables,\n",
    "        output_variables=output_variables,\n",
    "        verbose=verbose,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "vline_settings = {\n",
    "    \"horizontal_strategy\": \"text\",\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "hline_settings = {\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"vertical_strategy\": \"text\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "line_settings = {\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "text_settings = {\n",
    "    \"horizontal_strategy\": \"text\",\n",
    "    \"vertical_strategy\": \"text\",\n",
    "    \"intersection_x_tolerance\": 5,\n",
    "    \"snap_y_tolerance\": 5,\n",
    "    \"join_x_tolerance\": 5,\n",
    "    \"join_y_tolerance\": 5,\n",
    "}\n",
    "text_settings = {\n",
    "    # \"intersection_x_tolerance\": 5,\n",
    "    # \"snap_y_tolerance\": 5,\n",
    "    # \"join_x_tolerance\": 5,\n",
    "    # \"join_y_tolerance\": 5,\n",
    "    \"text_layout\": True\n",
    "}\n",
    "\n",
    "\n",
    "def get_fullest_row(table):\n",
    "    rows = [r for r in table if full_row(r)]\n",
    "    if len(rows) == 0:\n",
    "        rows = table\n",
    "    row = max(rows, key=len)\n",
    "    return row, table.index(row)\n",
    "\n",
    "\n",
    "def num_full_parts(row):\n",
    "    return len([p for p in row if not empty_part(p)])\n",
    "\n",
    "\n",
    "def get_table_items(table):\n",
    "    if table is None or len(table) == 0:\n",
    "        return []\n",
    "\n",
    "    cols, cols_idx = get_fullest_row(table)\n",
    "    for i, c in enumerate(cols):\n",
    "        if empty_part(c):\n",
    "            cols[i] = f\"col_{i}\"\n",
    "\n",
    "    # let's assume that the first full row after the cols row is the first item\n",
    "    first_order_row_idx = get_first_full_row(table[cols_idx + 1 :])[1]\n",
    "    if first_order_row_idx is None:\n",
    "        first_order_row_idx = get_first_non_empty_row(table[cols_idx + 1 :])[1]\n",
    "    if first_order_row_idx is None:\n",
    "        first_order_row_idx = 0\n",
    "    first_order_row_idx += cols_idx + 1\n",
    "\n",
    "    items = []\n",
    "    item = {c: \"\" for c in cols}\n",
    "    first_order_row_idx = min(first_order_row_idx, len(table) - 1)\n",
    "    order_table = table[first_order_row_idx:]\n",
    "    curr_row_len = num_full_parts(order_table[0])\n",
    "    for row in order_table:\n",
    "        if ((num_full_parts(row) == curr_row_len) or empty_row(row)) and len(item) > 0:\n",
    "            items.append(item)\n",
    "            item = {c: \"\" for c in cols}\n",
    "            if not empty_row(row):\n",
    "                curr_row_len = num_full_parts(row)\n",
    "        for i, c in enumerate(cols):\n",
    "            row_part = row[i]\n",
    "            if not empty_part(row_part):\n",
    "                row_part = \" \".join(row[i].split(\"\\n\"))\n",
    "                item[c] += row_part + \" \"\n",
    "    items.append(item)\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def row_check(row, target_list, target_thresh=2):\n",
    "    \"\"\"\n",
    "    Checks if the given row contains the target elements.\n",
    "\n",
    "    Parameters:\n",
    "        row (str): A string representing the row to check.\n",
    "        target_list (list): A list of strings representing the target elements.\n",
    "        target_thresh (int): The minimum number of target elements that must be present in the row.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the row contains the target elements, False otherwise.\n",
    "    \"\"\"\n",
    "    check_list = [hc for hc in target_list if hc.lower() in row.strip().lower()]\n",
    "    return len(check_list) >= target_thresh\n",
    "\n",
    "\n",
    "def extract_sub_text(\n",
    "    text,\n",
    "    top_cols,\n",
    "    bottom_cols,\n",
    "    top_thresh=2,\n",
    "    bottom_thresh=1,\n",
    "    alt_top_index=0,\n",
    "    alt_bottom_index=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts the text between the top_cols and bottom_cols.\n",
    "    \"\"\"\n",
    "    top_idx = find_target_index(\n",
    "        text, top_cols, target_thresh=top_thresh, alt_index=alt_top_index\n",
    "    )\n",
    "    bottom_idx = find_target_index(\n",
    "        text[::-1], bottom_cols, target_thresh=bottom_thresh, alt_index=alt_bottom_index\n",
    "    )\n",
    "    bottom_idx = len(text) - bottom_idx\n",
    "    return (\n",
    "        [t for t in text[top_idx : bottom_idx + 1] if len(t.strip()) > 0],\n",
    "        top_idx,\n",
    "        bottom_idx,\n",
    "    )\n",
    "\n",
    "\n",
    "def find_target_index(data, target_list, target_thresh=2, alt_index=0):\n",
    "    \"\"\"\n",
    "    Finds the index of the row in the given data that contains the target elements.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): A list of strings representing the data.\n",
    "        target_list (list): A list of strings representing the target elements.\n",
    "        target_thresh (int): The minimum number of target elements that must be present in a row.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the row that contains the target elements. If no such row exists, returns alt_index.\n",
    "    \"\"\"\n",
    "    target_idx = None\n",
    "    for idx, row in enumerate(data):\n",
    "        if row_check(row, target_list, target_thresh):\n",
    "            target_idx = idx\n",
    "            break\n",
    "    if target_idx is None:\n",
    "        msg.warn(f\"No target found in data. Setting it to {alt_index}.\", spaced=True)\n",
    "        target_idx = alt_index\n",
    "    return target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def json_str(x):\n",
    "    x = x[x.find(\"{\") : x.rfind(\"}\") + 1]\n",
    "    x = x.splitlines()\n",
    "    jstr = [x[0]]\n",
    "    for s in x[1:]:\n",
    "        if \":\" not in s and s.strip() != \"}\":\n",
    "            jstr[-1] = jstr[-1][:-2] + \" \" + s[1:]\n",
    "        else:\n",
    "            jstr.append(s)\n",
    "    jstr = \" \".join(jstr).strip()\n",
    "    if jstr[-1] in [\",\", \";\"]:\n",
    "        jstr = jstr[:-1].strip()\n",
    "    jstr = jstr.replace(\",}\", \"}\")\n",
    "    jstr = re.sub(r\"\\s+\", \" \", jstr)\n",
    "    jstr = jstr.replace('\" ', '\"')\n",
    "    jstr = jstr.replace(' \"', '\"')\n",
    "    jstr = jstr.replace(\", }\", \"}\")\n",
    "    jstr = re.sub(r\"[a-zA-Z0-9]\\}\", '\"}', jstr)\n",
    "    jstr = re.sub(r\":\\s*(\\w)\", r': \"\\1', jstr)\n",
    "    jstr = re.sub(r\"\\b0+(\\d+)\\b\", r\"\\1\", jstr)\n",
    "    # jstr = re.sub(r\"\\s*([{}])\\s*\", r\"\\1\", jstr)\n",
    "    jstr = re.sub(r\"\\s*([:,])\\s*\", r\"\\1 \", jstr)\n",
    "    jstr = re.sub(r\",}\", '\"}', jstr)\n",
    "    jstr = re.sub(r\"\\\"+}\", '\"}', jstr)\n",
    "    jstr = jstr.replace('\"\"', '\",\"')\n",
    "    return jstr\n",
    "\n",
    "\n",
    "def str_to_json(x, max_try=10):\n",
    "    # jstr = json_str(x)\n",
    "    jstr = x[x.find(\"{\") : x.rfind(\"}\") + 1]\n",
    "    jstr = jstr.replace(': \",', ': \"\",')\n",
    "    json_dict = {}\n",
    "    tries = 0\n",
    "    while True and tries < max_try:\n",
    "        try:\n",
    "            json_dict = json.loads(jstr)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            unexp = int(re.findall(r\"\\(char (\\d+)\\)\", str(e))[0])\n",
    "            unesc = jstr.rfind(r'\"', 0, unexp)\n",
    "            jstr = jstr[:unesc] + r\"\\\"\" + jstr[unesc + 1 :]\n",
    "            closg = jstr.find(r'\"', unesc + 2)\n",
    "            jstr = jstr[:closg] + r\"\\\"\" + jstr[closg + 1 :]\n",
    "            tries += 1\n",
    "    return jstr, json_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def extract_text(path):\n",
    "    data = pdfplumber.open(path)\n",
    "    pdf_text = [p.extract_text(layout=True).splitlines() for p in data.pages]\n",
    "    text = pdf_text[0]\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_order_docs(\n",
    "    text,\n",
    "    header_cols=[\"item\", \"description\", \"price\", \"quantity\", \"amount\", \"total\", \"qty\"],\n",
    "    get_parts=True,\n",
    "    splitter=None,\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "):\n",
    "    msg.info(\"Extracting ORDER docs from PDF.\", spaced=True)\n",
    "\n",
    "    if splitter is None:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "    avg_len = mode([len(t) for t in text])\n",
    "    order_text = [text[0]]\n",
    "    order_metadatas = [{}]\n",
    "    desc = \"\"\n",
    "    # for i, txt in enumerate(text[1:-1], start=1):\n",
    "    for txt in text[1:-1]:\n",
    "        if not len(txt) >= avg_len * 2:\n",
    "            txt = txt.replace('\"', \"\").replace(\"'\", \"\").strip()\n",
    "            if row_check(txt, header_cols, 2):\n",
    "                order_text.append(txt)\n",
    "                order_metadatas.append({})\n",
    "            elif len(txt) < avg_len * 0.75 and not row_check(\n",
    "                order_text[-1], header_cols, 2\n",
    "            ):\n",
    "                desc += \" \" + txt\n",
    "            else:\n",
    "                if len(desc) > 0:\n",
    "                    order_metadatas.append({\"desc\": desc.strip()})\n",
    "                    desc = \"\"\n",
    "                if get_parts:\n",
    "                    part_nums = [\n",
    "                        x\n",
    "                        for x in re.findall(r\"\\d{5}\", txt)\n",
    "                        if not x.startswith(\"00\") and \".\" not in x\n",
    "                    ]\n",
    "                    if len(part_nums) == 0:\n",
    "                        part_nums = [\n",
    "                            x\n",
    "                            for x in re.findall(r\"\\d{4}\", txt)\n",
    "                            if not x.startswith(\"00\") and \".\" not in x\n",
    "                        ]\n",
    "                    if len(part_nums) > 0:\n",
    "                        part_num = part_nums[0]\n",
    "                        txt += f\" part_number: {part_num}\"\n",
    "                order_text.append(txt)\n",
    "    if len(desc) > 0:\n",
    "        order_metadatas.append({\"desc\": desc.strip()})\n",
    "    order_text.append(text[-1])\n",
    "    order_metadatas += [{} for _ in range(len(order_text) - len(order_metadatas))]\n",
    "    docs = splitter.create_documents(order_text, metadatas=order_metadatas)\n",
    "    msg.good(\"ORDER docs extracted.\", spaced=True)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def info_order_docs(\n",
    "    text,\n",
    "    header_cols=[\"item\", \"description\", \"price\", \"quantity\", \"amount\", \"total\", \"qty\"],\n",
    "    total_cols=[\"total\", \"subtotal\", \"tax\"],\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "    get_parts=True,\n",
    "):\n",
    "    msg.info(\"Extracting INFO and ORDER docs from PDF.\", spaced=True)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    table_text, top_idx, bottom_idx = extract_sub_text(\n",
    "        text,\n",
    "        top_cols=header_cols,\n",
    "        bottom_cols=total_cols,\n",
    "        top_thresh=2,\n",
    "        bottom_thresh=1,\n",
    "        alt_top_index=0,\n",
    "        alt_bottom_index=0,\n",
    "    )\n",
    "\n",
    "    top_text = [\n",
    "        t.replace('\"', \"\").replace(\"'\", \"\").strip()\n",
    "        for t in text[:top_idx]\n",
    "        if len(t.strip()) > 0\n",
    "    ]\n",
    "    bottom_text = [\n",
    "        t.replace('\"', \"\").replace(\"'\", \"\").strip()\n",
    "        for t in text[bottom_idx:]\n",
    "        if len(t.strip()) > 0\n",
    "    ]\n",
    "    info_text = top_text + bottom_text[1:]\n",
    "    all_info_text = \" \".join(info_text)\n",
    "    qns = [x for x in re.findall(r\"\\d{8}\", all_info_text) if x.startswith(\"2\")]\n",
    "    if len(qns) > 0:\n",
    "        info_text.insert(0, f\"QOUTE NUMBER: {qns[0]}\")\n",
    "    info_docs = splitter.create_documents(info_text)\n",
    "    order_docs = extract_order_docs(\n",
    "        table_text, header_cols=header_cols, get_parts=get_parts, splitter=splitter\n",
    "    )\n",
    "    msg.good(\"INFO and ORDER docs extracted.\", spaced=True)\n",
    "\n",
    "    return dict(info_docs=info_docs, order_docs=order_docs)\n",
    "\n",
    "\n",
    "def pdf_to_info_order_docs(\n",
    "    path,\n",
    "    header_cols=[\"item\", \"description\", \"price\", \"quantity\", \"amount\", \"total\", \"qty\"],\n",
    "    total_cols=[\"total\", \"subtotal\", \"tax\"],\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "    get_parts=True,\n",
    "):\n",
    "    text = extract_text(path)\n",
    "    res = info_order_docs(\n",
    "        text,\n",
    "        header_cols=header_cols,\n",
    "        total_cols=total_cols,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        get_parts=get_parts,\n",
    "    )\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = default_device()\n",
    "\n",
    "# embeddings = SentenceTransformerEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "#     model_kwargs={\"device\": default_device()},\n",
    "# )\n",
    "# model = \"tiiuae/falcon-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def qa_llm_chain(model=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    token = \"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\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model, device_map=\"auto\", torch_dtype=torch.float16, token=token\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0})\n",
    "    return load_qa_chain(llm, \"stuff\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def fix_json(text):\n",
    "    json_str = copy.deepcopy(text)\n",
    "    if json_str.count(\"{\") != json_str.count(\"}\"):\n",
    "        while json_str.count(\"{\") % 2 != 0:\n",
    "            json_str = \"{\" + json_str\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "        while json_str.count(\"}\") % 2 != 0:\n",
    "            json_str = json_str + \"}\"\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "    if json_str.count(\"{\") < json_str.count(\"}\"):\n",
    "        while json_str.count(\"{\") != json_str.count(\"}\"):\n",
    "            json_str = \"{\" + json_str\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "    elif json_str.count(\"{\") > json_str.count(\"}\"):\n",
    "        while json_str.count(\"{\") != json_str.count(\"}\"):\n",
    "            json_str = json_str + \"}\"\n",
    "            json_str = json_str.replace(\"}{\", \"},{\")\n",
    "    json_str = json_str.replace(\"\\n\", \"\")\n",
    "    if json_str.startswith(\"{{\") and json_str.endswith(\"}}\"):\n",
    "        json_str = json_str[1:-1]\n",
    "    json_str = json_str.replace('\"\"\"', '\"')\n",
    "    json_str = json_str.replace('\"\"', '\",\"')\n",
    "    return json_str\n",
    "\n",
    "\n",
    "def json_response(chain, docs, query, max_tries=6):\n",
    "    msg.info(\"Converting DOCS to JSON.\", spaced=True)\n",
    "    overall_tries = 0\n",
    "    while overall_tries < max_tries / 2:\n",
    "        tries = 0\n",
    "        res = \"\"\n",
    "        while res == \"\" and tries < max_tries:\n",
    "            msg.info(f\"Tries: {tries}\", spaced=True)\n",
    "            res = chain(dict(input_documents=docs, question=query))\n",
    "            res = res[\"output_text\"].strip()\n",
    "            res = res[res.find(\"{\") : res.rfind(\"}\") + 1]\n",
    "            tries += 1\n",
    "        tries = 0\n",
    "        while tries < max_tries:\n",
    "            msg.info(f\"Tries: {tries}\", spaced=True)\n",
    "            try:\n",
    "                json_res = dict(json_str=res, json=json.loads(fix_json(res)))\n",
    "                msg.good(\"DOCS converted to JSON.\", spaced=True)\n",
    "                return json_res\n",
    "            except:\n",
    "                tries += 1\n",
    "        overall_tries += 1\n",
    "    msg.fail(\"Failed to convert DOCS to JSON.\", spaced=True)\n",
    "    return dict(json_str=res, json={})\n",
    "\n",
    "\n",
    "def info_json(chain, info_docs, max_tries=6):\n",
    "    msg.info(\"Extracting INFO JSON.\", spaced=True)\n",
    "    info_query = \"\"\"Extract the order information like the numbers, dates, and shipping address. Include the quote number too if found.\"\"\"\n",
    "    json_query = \"\"\"\\nReturn the text in JSON format. It must be compatible with json.loads.\"\"\"\n",
    "    suffix = \"\"\"\\nDon't tell me how to do it, just do it. Don't add any disclaimer.\"\"\"\n",
    "    info_query += json_query + suffix\n",
    "    res = json_response(chain, info_docs, info_query, max_tries)\n",
    "    msg.good(\"INFO JSON extracted.\", spaced=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def order_json(\n",
    "    chain,\n",
    "    order_docs,\n",
    "    max_tries=6,\n",
    "    get_parts=True,\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=0,\n",
    "):\n",
    "    msg.info(\"Extracting ORDER JSON.\", spaced=True)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    json_query = \"\"\"\\nReturn the text in JSON format. It must be compatible with json.loads.\"\"\"\n",
    "    suffix = \"\"\"\\nDon't tell me how to do it, just do it. Don't add any disclaimer.\"\"\"\n",
    "    part_query = \"\"\"Include the part numbers if defined.\"\"\"\n",
    "    query = \"\"\"Extract the order items with full details and descriptions and prices.\"\"\"\n",
    "    if get_parts:\n",
    "        query += \" \" + part_query\n",
    "    query += suffix\n",
    "    query = query.strip()\n",
    "    items = chain(dict(input_documents=order_docs, question=query))[\"output_text\"].strip()\n",
    "\n",
    "    item_query = json_query\n",
    "    if get_parts:\n",
    "        item_query += \" \" + part_query\n",
    "    item_query += suffix\n",
    "    item_query = item_query.strip()\n",
    "\n",
    "    item_docs = splitter.create_documents([items])\n",
    "    res = json_response(chain, item_docs, item_query, max_tries)\n",
    "    msg.good(\"ORDER JSON extracted.\", spaced=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def pdf_to_info_order_json(path, chain, max_tries=6, get_parts=True):\n",
    "    info_order_dict = pdf_to_info_order_docs(path, get_parts=get_parts)\n",
    "    info_dict = info_json(\n",
    "        chain=chain, info_docs=info_order_dict[\"info_docs\"], max_tries=max_tries\n",
    "    )\n",
    "    order_dict = order_json(\n",
    "        chain=chain,\n",
    "        order_docs=info_order_dict[\"order_docs\"],\n",
    "        max_tries=max_tries,\n",
    "        get_parts=get_parts,\n",
    "    )\n",
    "    return {\"info\": info_dict, \"order\": order_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369f85626145450c9d4eafdc18f413e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "llm_chain = qa_llm_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "data_path = Path(\"/home/hamza/demo_files/ap/\")\n",
    "data_path = Path(\"/home/hamza/demo_files/pdf\")\n",
    "file_name = \"wt13.pdf\"\n",
    "pdf = data_path / file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;4mℹ Extracting INFO and ORDER docs from PDF.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Extracting ORDER docs from PDF.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ ORDER docs extracted.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ INFO and ORDER docs extracted.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Extracting INFO JSON.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Converting DOCS to JSON.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ DOCS converted to JSON.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ INFO JSON extracted.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Extracting ORDER JSON.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Converting DOCS to JSON.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ DOCS converted to JSON.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;2m✔ ORDER JSON extracted.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "info_order_json = pdf_to_info_order_json(pdf, llm_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
